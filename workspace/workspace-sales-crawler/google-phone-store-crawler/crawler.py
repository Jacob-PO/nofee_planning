"""
ë‹¹ê·¼ë§ˆì¼“ íœ´ëŒ€í° ë§¤ì¥ í¬ë¡¤ëŸ¬ (Selenium ë²„ì „)
- Chrome ë¸Œë¼ìš°ì €ë¥¼ ì§ì ‘ ì œì–´
- ë‹¹ê·¼ë§ˆì¼“ ì‚¬ì´íŠ¸ì—ì„œ ë§¤ì¥ ê²€ìƒ‰
- ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© ì¶œë ¥
"""

import time
import re
from datetime import datetime
from pathlib import Path
import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

class DaangnStoreCrawlerSelenium:
    """ë‹¹ê·¼ë§ˆì¼“ íœ´ëŒ€í° ë§¤ì¥ í¬ë¡¤ëŸ¬ (Selenium)"""

    def __init__(self, google_api_key_path=None, headless=False):
        self.base_path = Path(__file__).parent
        self.output_path = self.base_path / 'output'
        self.output_path.mkdir(exist_ok=True)

        if google_api_key_path is None:
            self.google_api_key_path = self.base_path.parent.parent / 'config' / 'google_api_key.json'
        else:
            self.google_api_key_path = Path(google_api_key_path)

        # Chrome ì˜µì…˜ ì„¤ì •
        self.chrome_options = Options()
        if headless:
            self.chrome_options.add_argument('--headless')
        self.chrome_options.add_argument('--no-sandbox')
        self.chrome_options.add_argument('--disable-dev-shm-usage')
        self.chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        self.chrome_options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

        self.driver = None

        # ì „êµ­ ì§€ì—­
        self.regions = [
            'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ì„¸ì¢…',
            'ê²½ê¸°', 'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼'
        ]

        # ê²€ìƒ‰ í‚¤ì›Œë“œ
        self.keywords = [
            'íœ´ëŒ€í°ë§¤ì¥',
            'íœ´ëŒ€í°ì„±ì§€',
            'ìŠ¤ë§ˆíŠ¸í°ë§¤ì¥',
            'í°ë§¤ì¥',
            'ì¤‘ê³ í°',
            'ì•„ì´í°',
        ]

        self.results = []

    def init_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        try:
            print("ğŸŒ Chrome ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘...")
            self.driver = webdriver.Chrome(options=self.chrome_options)
            self.driver.set_window_size(1920, 1080)
            print("âœ… Chrome ë¸Œë¼ìš°ì € ì‹œì‘ ì™„ë£Œ")
            return True
        except Exception as e:
            print(f"âŒ Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            print("   chromedriverê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return False

    def close_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            print("ğŸ”´ Chrome ë¸Œë¼ìš°ì € ì¢…ë£Œ")

    def search_daangn_stores(self, keyword, region=None):
        """ë‹¹ê·¼ë§ˆì¼“ì—ì„œ ë§¤ì¥ ê²€ìƒ‰"""
        try:
            search_query = f"{region} {keyword}" if region else keyword
            print(f"  ğŸ” ê²€ìƒ‰: {search_query}")

            # Google ê²€ìƒ‰
            google_url = f"https://www.google.com/search?q=ë‹¹ê·¼ë§ˆì¼“+{search_query}+site:daangn.com"
            self.driver.get(google_url)
            time.sleep(3)

            # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë‹¹ê·¼ë§ˆì¼“ ë§í¬ ìˆ˜ì§‘
            daangn_links = []
            try:
                # ë‹¤ì–‘í•œ CSS ì…€ë ‰í„°ë¡œ ì‹œë„
                selectors = [
                    'a[href*="daangn.com"]',
                    'div.g a',
                    'a[jsname]',
                    'a',
                ]

                all_links = []
                for selector in selectors:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    for elem in elements:
                        url = elem.get_attribute('href')
                        if url and 'daangn.com' in url:
                            all_links.append(url)

                # ì¤‘ë³µ ì œê±°
                all_links = list(set(all_links))

                print(f"    ğŸ“Œ ì´ {len(all_links)}ê°œ daangn.com ë§í¬ ë°œê²¬")

                # local-profile ë§í¬ë§Œ í•„í„°ë§ (ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ì œì™¸)
                for url in all_links:
                    if 'local-profile' in url or 'business-profile' in url:
                        # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ëŠ” ì œì™¸ (?in= ì™€ search= ê°€ ìˆëŠ” ê²½ìš°)
                        if '?in=' in url and 'search=' in url:
                            continue
                        if url not in daangn_links:
                            daangn_links.append(url)
                            print(f"    âœ… í”„ë¡œí•„ ë§í¬ ë°œê²¬: {url[:80]}...")

                # í”„ë¡œí•„ ë§í¬ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ë‹¹ê·¼ë§ˆì¼“ ë§í¬ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                if not daangn_links and all_links:
                    print(f"    â„¹ï¸  ë°œê²¬ëœ ë§í¬ ìƒ˜í”Œ:")
                    for url in all_links[:5]:
                        print(f"      - {url[:100]}")

            except Exception as e:
                print(f"    âš ï¸  ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                import traceback
                traceback.print_exc()

            return daangn_links

        except Exception as e:
            print(f"    âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return []

    def extract_store_info_from_page(self, url):
        """ë‹¹ê·¼ë§ˆì¼“ í˜ì´ì§€ì—ì„œ ë§¤ì¥ ì •ë³´ ì¶”ì¶œ (ê°œë³„ ë§¤ì¥ í˜ì´ì§€ë§Œ)"""
        try:
            # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ëŠ” ê±´ë„ˆë›°ê¸°
            if '?in=' in url and 'search=' in url:
                print(f"    â­ï¸  ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ - ê±´ë„ˆëœ€")
                return None

            print(f"    ğŸ“„ í˜ì´ì§€ ë¶„ì„ ì¤‘...")
            self.driver.get(url)
            time.sleep(3)

            # ê°œë³„ ë§¤ì¥ í˜ì´ì§€ë§Œ ì²˜ë¦¬
            if False:  # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ ì²˜ë¦¬ ë¹„í™œì„±í™”
                # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ - ì—¬ëŸ¬ ë§¤ì¥ ë¦¬ìŠ¤íŠ¸
                print(f"      ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ - ë§¤ì¥ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ")

                try:
                    # ë§¤ì¥ ë¦¬ìŠ¤íŠ¸ ì•„ì´í…œ ì°¾ê¸° (XPath ì‚¬ìš©)
                    store_items = self.driver.find_elements(By.XPATH, '//ul/li')

                    print(f"      ğŸ“Œ {len(store_items)}ê°œ ì•„ì´í…œ ë°œê²¬")

                    count = 0
                    for idx, item in enumerate(store_items[:20], 1):  # ìƒìœ„ 20ê°œë§Œ
                        try:
                            # ë§í¬ ì°¾ê¸°
                            try:
                                link_elem = item.find_element(By.CSS_SELECTOR, 'a[href*="local-profile"]')
                                store_url = link_elem.get_attribute('href')
                            except:
                                continue

                            # ì „ì²´ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
                            item_text = item.text

                            # ë””ë²„ê¹…: ì²« ë²ˆì§¸ ì•„ì´í…œ ì¶œë ¥
                            if idx == 1 and item_text:
                                print(f"      [ë””ë²„ê·¸] ì²« ë²ˆì§¸ ì•„ì´í…œ í…ìŠ¤íŠ¸:\n{item_text[:200]}")

                            # ì „í™”ë²ˆí˜¸ ë¨¼ì € í™•ì¸
                            phone_pattern = r'010-?\d{3,4}-?\d{4}'
                            phones = re.findall(phone_pattern, item_text)

                            if not phones:
                                continue

                            # ë§¤ì¥ëª… ì¶”ì¶œ
                            store_name = item_text.split('\n')[0].strip()

                            if not store_name or len(store_name) > 50:
                                continue

                            # ì§€ì—­ ì¶”ì¶œ
                            from urllib.parse import unquote
                            region = 'ì§€ì—­ ë¯¸í™•ì¸'
                            if '?in=' in url:
                                region_param = url.split('?in=')[1].split('&')[0]
                                region = unquote(region_param)

                            if phones:
                                # ì „í™”ë²ˆí˜¸ ì •ê·œí™”
                                normalized_phones = []
                                for phone in phones:
                                    digits = re.sub(r'[^0-9]', '', phone)
                                    if len(digits) == 10:
                                        formatted = f"{digits[:3]}-{digits[3:6]}-{digits[6:]}"
                                    elif len(digits) == 11:
                                        formatted = f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"
                                    else:
                                        formatted = phone
                                    normalized_phones.append(formatted)

                                unique_phones = list(set(normalized_phones))

                                results.append({
                                    'store_name': store_name,
                                    'phones': unique_phones,
                                    'region': region,
                                    'url': store_url
                                })
                                count += 1
                                print(f"        [{count}] {store_name}: {unique_phones[0]}")

                        except Exception as e:
                            continue

                    print(f"      âœ… ì´ {count}ê°œ ë§¤ì¥ ì „í™”ë²ˆí˜¸ ì¶”ì¶œ")

                except Exception as e:
                    print(f"      âš ï¸  ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                    import traceback
                    traceback.print_exc()

                return results if results else None

            else:
                # ê°œë³„ ë§¤ì¥ í˜ì´ì§€
                page_text = self.driver.find_element(By.TAG_NAME, 'body').text

                # ë§¤ì¥ëª… ì¶”ì¶œ - XPath ì‚¬ìš©
                store_name = "ë§¤ì¥ëª… ë¯¸í™•ì¸"
                try:
                    # XPathë¡œ ë§¤ì¥ëª… ì°¾ê¸°
                    store_element = self.driver.find_element(By.XPATH, '/html/body/main/div[1]/div[2]/div[1]/h1')
                    store_name = store_element.text.strip()
                    print(f"    âœ… ë§¤ì¥ëª…: {store_name}")
                except:
                    # ëŒ€ì²´ ë°©ë²•: h1 íƒœê·¸ì—ì„œ ì°¾ê¸°
                    try:
                        title_elements = self.driver.find_elements(By.CSS_SELECTOR, 'h1')
                        if title_elements:
                            store_name = title_elements[0].text.strip()
                    except:
                        pass

                # URLì—ì„œë„ ì‹œë„
                if store_name == "ë§¤ì¥ëª… ë¯¸í™•ì¸":
                    url_parts = url.split('local-profile/')
                    if len(url_parts) > 1:
                        store_part = url_parts[1].split('-')[0]
                        from urllib.parse import unquote
                        store_name = unquote(store_part)

                # ì „í™”ë²ˆí˜¸ ì¶”ì¶œ
                phone_pattern = r'010-?\d{3,4}-?\d{4}'
                phones = re.findall(phone_pattern, page_text)

                # ì •ê·œí™”
                normalized_phones = []
                for phone in phones:
                    digits = re.sub(r'[^0-9]', '', phone)
                    if len(digits) == 10:
                        formatted = f"{digits[:3]}-{digits[3:6]}-{digits[6:]}"
                    elif len(digits) == 11:
                        formatted = f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"
                    else:
                        formatted = phone
                    normalized_phones.append(formatted)

                # ì¤‘ë³µ ì œê±°
                unique_phones = list(set(normalized_phones))

                # ì§€ì—­ ì¶”ì¶œ (ìƒì„¸ - ì‹œ/ë„ + êµ¬/êµ°)
                region = 'ì§€ì—­ ë¯¸í™•ì¸'

                # URLì—ì„œ ì§€ì—­ ì¶”ì¶œ ì‹œë„
                if '?in=' in url:
                    from urllib.parse import unquote
                    region_param = url.split('?in=')[1].split('&')[0]
                    region = unquote(region_param)

                # í˜ì´ì§€ í…ìŠ¤íŠ¸ì—ì„œ ìƒì„¸ ì§€ì—­ ì¶”ì¶œ
                if region == 'ì§€ì—­ ë¯¸í™•ì¸':
                    # "ì„œìš¸ ê°•ë‚¨êµ¬", "ë¶€ì‚° í•´ìš´ëŒ€êµ¬" ê°™ì€ íŒ¨í„´ ì°¾ê¸°
                    region_pattern = r'(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼)\s*([ê°€-í£]+[ì‹œêµ°êµ¬])'
                    region_match = re.search(region_pattern, page_text)
                    if region_match:
                        region = f"{region_match.group(1)} {region_match.group(2)}"
                    else:
                        # ë‹¨ìˆœíˆ ì‹œ/ë„ë§Œ ì°¾ê¸°
                        for r in self.regions:
                            if r in page_text:
                                region = r
                                break

                if unique_phones:
                    print(f"    âœ… ë§¤ì¥: {store_name}, ì „í™”ë²ˆí˜¸: {len(unique_phones)}ê°œ, ì§€ì—­: {region}")
                    return [{
                        'store_name': store_name,
                        'phones': unique_phones,
                        'region': region,
                        'url': url
                    }]
                else:
                    print(f"    âš ï¸  ì „í™”ë²ˆí˜¸ ì—†ìŒ")
                    return None

        except Exception as e:
            print(f"    âŒ í˜ì´ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

    def crawl(self, max_searches=30):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        print("=" * 80)
        print("ğŸ¥• ë‹¹ê·¼ë§ˆì¼“ íœ´ëŒ€í° ë§¤ì¥ í¬ë¡¤ëŸ¬ (Selenium)")
        print("=" * 80)

        # ë“œë¼ì´ë²„ ì´ˆê¸°í™”
        if not self.init_driver():
            return []

        all_results = []
        visited_urls = set()

        try:
            search_count = 0

            # ì§€ì—­ë³„ í‚¤ì›Œë“œ ì¡°í•©
            for region in self.regions:
                if search_count >= max_searches:
                    break

                for keyword in self.keywords:
                    if search_count >= max_searches:
                        break

                    print(f"\n[{search_count + 1}/{max_searches}] ğŸ” {region} {keyword}")

                    # ê²€ìƒ‰
                    daangn_links = self.search_daangn_stores(keyword, region)

                    if not daangn_links:
                        print(f"    âš ï¸  ë§í¬ ì—†ìŒ")
                        search_count += 1
                        continue

                    print(f"    ğŸ“ {len(daangn_links)}ê°œ ë§í¬ ë°œê²¬")

                    # ê° ë§í¬ ë°©ë¬¸í•˜ì—¬ ì •ë³´ ì¶”ì¶œ
                    for link in daangn_links:
                        if link in visited_urls:
                            continue

                        visited_urls.add(link)
                        store_info_list = self.extract_store_info_from_page(link)

                        # ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ë¨ (ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì¸ ê²½ìš° ì—¬ëŸ¬ ë§¤ì¥)
                        if store_info_list:
                            for store_info in store_info_list:
                                if store_info.get('phones'):
                                    for phone in store_info['phones']:
                                        result = {
                                            'ì§€ì—­ëª…': store_info['region'],
                                            'ë§¤ì¥ëª…': store_info['store_name'],
                                            'ì§€ì—­ëª…_ë§¤ì¥ëª…': f"{store_info['region']}_{store_info['store_name']}",
                                            'ì „í™”ë²ˆí˜¸': phone,
                                            'ë§í¬': store_info['url']
                                        }
                                        all_results.append(result)
                                        print(f"      ğŸ’¾ ì €ì¥: {result['ë§¤ì¥ëª…']} ({phone})")

                        time.sleep(2)  # ìš”ì²­ ê°„ê²©

                    search_count += 1
                    time.sleep(3)  # ê²€ìƒ‰ ê°„ê²©

            self.results = all_results
            print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_results)}ê°œ ë§¤ì¥ ì •ë³´ ìˆ˜ì§‘")
            print(f"   ê³ ìœ  URL: {len(visited_urls)}ê°œ")

        except KeyboardInterrupt:
            print("\n\nâš ï¸  ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
            self.results = all_results

        except Exception as e:
            print(f"\nâŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            self.results = all_results

        finally:
            self.close_driver()

        return all_results

    def save_to_csv(self, filename=None):
        """CSV ì €ì¥"""
        if not self.results:
            print("âš ï¸  ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'daangn_stores_selenium_{timestamp}.csv'

        output_file = self.output_path / filename

        df = pd.DataFrame(self.results)

        # ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates(subset=['ì „í™”ë²ˆí˜¸', 'ë§í¬'], keep='first')

        # ì •ë ¬
        df = df.sort_values(by=['ì§€ì—­ëª…', 'ë§¤ì¥ëª…'])

        df.to_csv(output_file, index=False, encoding='utf-8-sig')

        print(f"ğŸ’¾ CSV ì €ì¥: {output_file}")
        print(f"   ì´ {len(df)}ê°œ (ì¤‘ë³µ ì œê±° í›„)")

        return output_file

    def upload_to_sheets(self, spreadsheet_url, worksheet_name='ë‹¹ê·¼ë§¤ì¥_Selenium'):
        """Google Sheets ì—…ë¡œë“œ"""
        if not self.results:
            print("âš ï¸  ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False

        try:
            print("\nğŸ“¤ Google Sheets ì—…ë¡œë“œ...")

            scopes = [
                'https://www.googleapis.com/auth/spreadsheets',
                'https://www.googleapis.com/auth/drive'
            ]

            creds = Credentials.from_service_account_file(
                str(self.google_api_key_path),
                scopes=scopes
            )

            client = gspread.authorize(creds)

            sheet_id = spreadsheet_url.split('/d/')[1].split('/')[0]
            spreadsheet = client.open_by_key(sheet_id)

            try:
                worksheet = spreadsheet.worksheet(worksheet_name)
            except:
                worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows=3000, cols=10)
                print(f"  âœ… ì›Œí¬ì‹œíŠ¸ '{worksheet_name}' ìƒì„±")

            df = pd.DataFrame(self.results)
            df = df.drop_duplicates(subset=['ì „í™”ë²ˆí˜¸', 'ë§í¬'], keep='first')

            print(f"  ğŸ“Š ì—…ë¡œë“œ: {len(df)}ê°œ")

            # ê¸°ì¡´ ë°ì´í„° í™•ì¸
            existing_data = worksheet.get_all_values()

            # í—¤ë” í¬í•¨ ì‘ì„±
            if len(existing_data) == 0:
                all_data = [df.columns.tolist()] + df.values.tolist()
                batch_size = 100
                worksheet.update('A1', all_data[:batch_size], value_input_option='RAW')
                print(f"  âœ… {min(len(all_data), batch_size)}ê°œ í–‰ ì‘ì„±")

                # ë‚˜ë¨¸ì§€
                remaining = all_data[batch_size:]
                if remaining:
                    time.sleep(60)
                    for i in range(0, len(remaining), batch_size):
                        batch = remaining[i:i+batch_size]
                        worksheet.append_rows(batch, value_input_option='RAW')
                        print(f"  âœ… {len(batch)}ê°œ í–‰ ì¶”ê°€")
                        time.sleep(60)
            else:
                # ì¶”ê°€
                new_rows = df.values.tolist()
                batch_size = 100
                for i in range(0, len(new_rows), batch_size):
                    batch = new_rows[i:i+batch_size]
                    worksheet.append_rows(batch, value_input_option='RAW')
                    print(f"  âœ… {len(batch)}ê°œ í–‰ ì¶”ê°€")
                    if i + batch_size < len(new_rows):
                        time.sleep(60)

            print(f"\nğŸ“Š ì™„ë£Œ: {spreadsheet_url}")
            return True

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½"""
        if not self.results:
            print("âš ï¸  ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        df = pd.DataFrame(self.results)
        df_unique = df.drop_duplicates(subset=['ì „í™”ë²ˆí˜¸'])

        print("\n" + "=" * 80)
        print("ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print("=" * 80)
        print(f"ì´ ìˆ˜ì§‘: {len(df)}ê°œ")
        print(f"ê³ ìœ  ì „í™”ë²ˆí˜¸: {len(df_unique)}ê°œ")
        print(f"ê³ ìœ  ì§€ì—­: {df['ì§€ì—­ëª…'].nunique()}ê°œ")

        print("\nğŸ“ ì§€ì—­ë³„ ë§¤ì¥ ìˆ˜:")
        region_counts = df_unique['ì§€ì—­ëª…'].value_counts().head(15)
        for region, count in region_counts.items():
            print(f"   {region}: {count}ê°œ")

        print("\nğŸ“Œ ìƒ˜í”Œ ë°ì´í„°:")
        print(df_unique.head(10).to_string(index=False, max_colwidth=40))


def main():
    """ë©”ì¸ ì‹¤í–‰"""

    SPREADSHEET_URL = "https://docs.google.com/spreadsheets/d/1IDbMaZucrE78gYPK_dhFGFWN_oixcRhlM1sU9tZMJRo/edit?gid=1073623102#gid=1073623102"

    # headless=Falseë¡œ í•˜ë©´ ë¸Œë¼ìš°ì €ê°€ ë³´ì„
    crawler = DaangnStoreCrawlerSelenium(headless=False)

    # í¬ë¡¤ë§ (í…ŒìŠ¤íŠ¸: 3ê°œ ê²€ìƒ‰)
    results = crawler.crawl(max_searches=3)

    # ìš”ì•½
    crawler.print_summary()

    # CSV ì €ì¥
    crawler.save_to_csv()

    # Google Sheets ì—…ë¡œë“œ
    if results:
        print("\nâ³ Google Sheets ì—…ë¡œë“œ ì¤‘...")
        crawler.upload_to_sheets(SPREADSHEET_URL, worksheet_name='google')

    print("\n" + "=" * 80)
    print("âœ… ì™„ë£Œ!")
    print("=" * 80)


if __name__ == "__main__":
    main()
